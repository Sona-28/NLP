{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization Functions"
      ],
      "metadata": {
        "id": "vpM2anIVHdyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "l6UAjY3KIWkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "aKyE5xmBKcJQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "gghLg13PHjN-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I'm Sona. I'm Studying B.E CSE...\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "print(tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDoispvQH0Vi",
        "outputId": "f99290f1-4972-4319-ee05-ca42e96e5173"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', \"'m\", 'Sona.', 'I', \"'m\", 'Studying', 'B.E', 'CSE', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. WordPunct Tokenizer"
      ],
      "metadata": {
        "id": "aSsQek0kIYW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "5cVWARO_Idsa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wordpunct_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWOuA_6IIlo3",
        "outputId": "f3a38a5a-fc34-42c1-ad0d-fcc016469d10"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', \"'\", 'm', 'Sona', '.', 'I', \"'\", 'm', 'Studying', 'B', '.', 'E', 'CSE', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. MWE Tokenizer"
      ],
      "metadata": {
        "id": "JZgGLbH-IzO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "V7DYvi5KI2sr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKR4AHKkKO35",
        "outputId": "6e6e9a10-e3ba-4b0e-931e-e0a9195a417c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokeniz = MWETokenizer()\n",
        "tokeniz.add_mwe(('B.E','CSE'))\n",
        "print(tokeniz.tokenize(word_tokenize(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ljlm8x3I5Yc",
        "outputId": "a9826f9f-bb4d-40fb-f229-a1b58d1c97f7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', \"'m\", 'Sona', '.', 'I', \"'m\", 'Studying', 'B.E_CSE', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Stemming  examples"
      ],
      "metadata": {
        "id": "TeScBx2PL6Lt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Lancaster Stemming"
      ],
      "metadata": {
        "id": "gimIV3WuL9IY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer"
      ],
      "metadata": {
        "id": "mixkIKk8L_9q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lancaster = LancasterStemmer()\n",
        "words = ['eating','eats','eaten','puts','putting']\n",
        "for word in words:\n",
        "    print(word,\"--->\",lancaster.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlFROv9zMTyj",
        "outputId": "386ce32e-75da-4c16-8084-e8d9f89985c0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating ---> eat\n",
            "eats ---> eat\n",
            "eaten ---> eat\n",
            "puts ---> put\n",
            "putting ---> put\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Regexp Stemmer"
      ],
      "metadata": {
        "id": "ZUDUgE0nMzGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer"
      ],
      "metadata": {
        "id": "wvhj95VnM3ju"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "words = ['massable','was','bees','computers','advisable']\n",
        "for word in words:\n",
        "    print(word,\"--->\",regexp.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA8N87MwM9Ww",
        "outputId": "27da6f5f-ae78-4026-94c9-8b0aecbbb283"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "massable ---> mass\n",
            "was ---> was\n",
            "bees ---> bee\n",
            "computers ---> computer\n",
            "advisable ---> advis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization techniques"
      ],
      "metadata": {
        "id": "CkfDm1o7NP8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. spaCy Lemmatization"
      ],
      "metadata": {
        "id": "Ysts0gR2OUBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "7eMaugaBNybL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(u'the bats saw the cats with best stripes hanging upside down by their feet')\n",
        "tokens = []\n",
        "for token in doc:\n",
        "    tokens.append(token)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST15PN4uOdZJ",
        "outputId": "e267ef3b-c590-48d7-bc66-005991d4a3ab"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[the, bats, saw, the, cats, with, best, stripes, hanging, upside, down, by, their, feet]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
        "print(lemmatized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZuX1JYVOqw7",
        "outputId": "045f412c-b10f-4021-9c04-f03dfcece6d5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the bat see the cat with good stripe hang upside down by -PRON- foot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. TreeTagger Lemmatization"
      ],
      "metadata": {
        "id": "KDUMgwGCOxYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pattern\n",
        "import pattern\n",
        "from pattern.en import lemma, lexeme\n",
        "from pattern.en import parse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJY0j4eJO1RA",
        "outputId": "e0fa9979-102c-44dd-e725-4607408cc78a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pattern\n",
            "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.2 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pattern) (0.16.0)\n",
            "Collecting backports.csv\n",
            "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
            "Collecting mysqlclient\n",
            "  Downloading mysqlclient-2.1.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from pattern) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pattern) (4.2.6)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.0 MB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "  Downloading pdfminer.six-20220319-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 16.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pattern) (3.2.5)\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 21.4 MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "  Downloading CherryPy-18.6.1-py2.py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 63.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pattern) (2.23.0)\n",
            "Collecting cheroot>=8.2.1\n",
            "  Downloading cheroot-8.6.0-py2.py3-none-any.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 70.0 MB/s \n",
            "\u001b[?25hCollecting zc.lockfile\n",
            "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (8.12.0)\n",
            "Collecting jaraco.collections\n",
            "  Downloading jaraco.collections-3.5.1-py3-none-any.whl (10 kB)\n",
            "Collecting portend>=2.1.1\n",
            "  Downloading portend-3.1.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting jaraco.functools\n",
            "  Downloading jaraco.functools-3.5.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (1.15.0)\n",
            "Collecting tempora>=1.8\n",
            "  Downloading tempora-5.0.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2022.1)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Collecting jaraco.text\n",
            "  Downloading jaraco.text-3.7.0-py3-none-any.whl (8.6 kB)\n",
            "Collecting jaraco.classes\n",
            "  Downloading jaraco.classes-3.2.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting jaraco.context>=4.1\n",
            "  Downloading jaraco.context-4.1.1-py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (5.7.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->jaraco.text->jaraco.collections->cherrypy->pattern) (3.8.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (3.0.4)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-37.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 41.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->pattern) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->pattern) (2.21)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (1.24.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zc.lockfile->cherrypy->pattern) (57.4.0)\n",
            "Building wheels for collected packages: pattern, mysqlclient, python-docx, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332721 sha256=6ec9376b93d5e993667c94233cfe7575f4c2b6d15d913b8ca46f47a3ed66e560\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/1f/4e/9b67afd2430d55dee90bd57618dd7d899f1323e5852c465682\n",
            "  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.1.0-cp37-cp37m-linux_x86_64.whl size=99956 sha256=1a020a9d34a2b8ede38b043ffb2f49dd2fd9abeb3c119402e59b000cc712aac2\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/d4/df/08cd6e1fa4a8691b268ab254bd0fa589827ab5b65638c010b4\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=6e9f4c8dea1e2832729fa654f7db4d7ee7e488aa1487d9b05ec009b56d533eef\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=2e123755eccd502606621fb4200be438ed78af6e528532aa86492aaff62b92c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built pattern mysqlclient python-docx sgmllib3k\n",
            "Installing collected packages: jaraco.functools, jaraco.context, tempora, jaraco.text, jaraco.classes, zc.lockfile, sgmllib3k, portend, jaraco.collections, cryptography, cheroot, python-docx, pdfminer.six, mysqlclient, feedparser, cherrypy, backports.csv, pattern\n",
            "Successfully installed backports.csv-1.0.7 cheroot-8.6.0 cherrypy-18.6.1 cryptography-37.0.0 feedparser-6.0.8 jaraco.classes-3.2.1 jaraco.collections-3.5.1 jaraco.context-4.1.1 jaraco.functools-3.5.0 jaraco.text-3.7.0 mysqlclient-2.1.0 pattern-3.6 pdfminer.six-20220319 portend-3.1.0 python-docx-0.8.11 sgmllib3k-1.0.0 tempora-5.0.1 zc.lockfile-2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"the bats saw the cats with best stripes hanging upside down by their feet\"\n",
        "lemmatized_sentence = \" \".join([lemma(word) for word in sentence.split()])\n",
        "print(lemmatized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnC5UGksQX03",
        "outputId": "644eda62-86d9-40d1-ff53-e4aca7e72964"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the bat see the cat with best stripe hang upside down by their feet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_lemmas_for_each_word = [lexeme(wd) for wd in sentence.split()]\n",
        "print(all_lemmas_for_each_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ds-LLv3ROso",
        "outputId": "92cd997b-58f5-4e86-aa70-6c161c15469d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['the', 'thes', 'thing', 'thed'], ['bat', 'bats', 'batting', 'batted'], ['see', 'sees', 'seeing', 'saw', 'seen'], ['the', 'thes', 'thing', 'thed'], ['cat', 'cats', 'catting', 'catted'], ['with', 'withs', 'withing', 'withed'], ['best', 'bests', 'besting', 'bested'], ['stripe', 'stripes', 'striping', 'striped'], ['hang', 'hangs', 'hanging', 'hung'], ['upside', 'upsides', 'upsiding', 'upsided'], ['down', 'downs', 'downing', 'downed'], ['by', 'bies', 'bying', 'bied'], ['their', 'theirs', 'theiring', 'theired'], ['feet', 'feets', 'feeting', 'feeted']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Genism"
      ],
      "metadata": {
        "id": "O4SuRDeuRRGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import lemmatize"
      ],
      "metadata": {
        "id": "bk_b8uYNRXrA"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"the bats saw the cats with best stripes hanging upside down by their feet\"\n",
        " \n",
        "lemmatized_sentence = [word.decode('utf-8').split('.')[0] for word in lemmatize(sentence)]\n",
        " \n",
        "print(lemmatized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO8QKl84RbS3",
        "outputId": "3d1d3355-bc37-4fff-a02d-ca1880e6492b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bat/NN', 'see/VB', 'cat/NN', 'best/JJ', 'stripe/NN', 'hang/VB', 'upside/RB', 'foot/NN']\n"
          ]
        }
      ]
    }
  ]
}